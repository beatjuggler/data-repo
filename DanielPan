import requests
from bs4 import BeautifulSoup, SoupStrainer
import bs4 # import main module first
import urllib2


def scrape_loop(search_link):
    r = requests.get(search_link)
    raw_html = r.text
    soup = BeautifulSoup(raw_html, 'html.parser')

    results_links = soup.find_all('a', {'class': 'hdrlnk'})

    for index, result in enumerate(results_links):   #alternative with indexing via enumerate
        result = result.get('href')
        concat_link = "http://taipei.craigslist.com.tw" + result
        print index +1, concat_link
		
        soup = BeautifulSoup(urllib2.urlopen(concat_link).read())
        postdate = soup.find_all('time')
        print "Time Posted: " + postdate[0]['datetime'] + '\n'
        
        if index == 99:
            return True
    return False


def recursive_scrape(search_link, page_count=1): #Loop for URL of next page button, only TW craigslist apa section has more than one page...
    
    r = requests.get(search_link)
    raw_html = r.text
    #soup = BeautifulSoup(raw_html, 'html.parser')
    
    if scrape_loop(search_link):    
           
        search_link = "http://taipei.craigslist.com.tw/search/apa" + "?s=" + str(page_count*100) + "&"
        print "Next page of results:" + '\n'
        
        recursive_scrape(search_link, page_count + 1)

if __name__ == '__main__':
    search_link = raw_input('Copy and paste your URL after searching here:') 
    recursive_scrape(search_link)
